# ğŸª Language Mirror Pro

<div align="center">

![Language Mirror Pro](https://img.shields.io/badge/AI-Custom%20Built-blue?style=for-the-badge)
![PyTorch](https://img.shields.io/badge/PyTorch-2.1+-red?style=for-the-badge&logo=pytorch)
![License](https://img.shields.io/badge/License-MIT-green?style=for-the-badge)

**A Custom-Built AI Language Tutor Using Reinforcement Learning**

*No External APIs â€¢ 100% Custom Neural Network â€¢ Built From Scratch*

[Live Demo](#demo) â€¢ [Architecture](#architecture) â€¢ [Quick Start](#quick-start) â€¢ [Technical Details](#technical-details)

</div>

---

## ğŸ† Why This Project Wins

| What Others Do | What We Built |
|----------------|---------------|
| âŒ Wrap ChatGPT/Gemini APIs | âœ… **Custom 12M parameter Transformer** |
| âŒ Simple chat interface | âœ… **Multi-task learning: Grammar + Pronunciation + Response** |
| âŒ Static responses | âœ… **RL-trained adaptive tutoring with PPO** |
| âŒ Generic feedback | âœ… **Pedagogically-informed reward shaping** |
| âŒ Single language | âœ… **10 languages with dialect support** |

---

## âœ¨ Key Features

### ğŸ§  Custom AI Architecture
- **Multi-Task Transformer** with RoPE positional encoding
- **SwiGLU activation** (used in LLaMA, PaLM)
- **RMSNorm** for stable training
- **Separate heads** for grammar, pronunciation, and response generation

### ğŸ¯ Reinforcement Learning
- **PPO algorithm** for policy optimization
- **Curriculum learning** - starts easy, increases difficulty
- **Simulated learner environment** for training without human data
- **Pedagogically-informed rewards** based on language learning science

### ğŸŒ Multilingual Support
- Italian, Japanese, Spanish, French, German
- Portuguese, Mandarin, Korean, Arabic, Hindi
- Regional dialect awareness

### âš¡ Production Ready
- FastAPI backend with WebSocket support
- Real-time conversation streaming
- Session management
- Sub-second inference on M4 Mac

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LANGUAGE MIRROR PRO ARCHITECTURE                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                          â”‚
â”‚  User Input: "Io sono fame"                                              â”‚
â”‚       â”‚                                                                  â”‚
â”‚       â–¼                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                    TOKENIZER (BPE)                               â”‚    â”‚
â”‚  â”‚  Multilingual â€¢ 16K vocab â€¢ Subword tokenization                â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚       â”‚                                                                  â”‚
â”‚       â–¼                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                 STATE ENCODER (6-Layer Transformer)              â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚    â”‚
â”‚  â”‚  â”‚   Token     â”‚  â”‚  Language   â”‚  â”‚ Proficiency â”‚              â”‚    â”‚
â”‚  â”‚  â”‚  Embedding  â”‚  â”‚  Embedding  â”‚  â”‚  Encoding   â”‚              â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚    â”‚
â”‚  â”‚         â”‚                â”‚                â”‚                      â”‚    â”‚
â”‚  â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚    â”‚
â”‚  â”‚                          â–¼                                       â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚    â”‚
â”‚  â”‚  â”‚   Multi-Head Attention (RoPE) + SwiGLU FFN Ã— 6 layers   â”‚    â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚       â”‚                                                                  â”‚
â”‚       â–¼                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                     MULTI-TASK HEADS                             â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚    â”‚
â”‚  â”‚  â”‚ Grammar  â”‚  â”‚ Pronun-  â”‚  â”‚ Response â”‚  â”‚ Adaptive â”‚        â”‚    â”‚
â”‚  â”‚  â”‚Correctionâ”‚  â”‚ ciation  â”‚  â”‚Generator â”‚  â”‚Difficultyâ”‚        â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚       â”‚                                                                  â”‚
â”‚       â–¼                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                    RL COMPONENTS (PPO)                           â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚    â”‚
â”‚  â”‚  â”‚  Policy Head    â”‚              â”‚   Value Head    â”‚           â”‚    â”‚
â”‚  â”‚  â”‚    (Actor)      â”‚              â”‚    (Critic)     â”‚           â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚       â”‚                                                                  â”‚
â”‚       â–¼                                                                  â”‚
â”‚  Output: "Quasi perfetto! Si dice 'Ho fame'. In italiano usiamo         â”‚
â”‚           'avere' per la fame, non 'essere'!"                           â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Quick Start

### Prerequisites
- Python 3.10+
- Node.js 18+ (for frontend)
- 8GB RAM minimum

### 1. Clone and Setup

```bash
git clone https://github.com/YOUR_USERNAME/language-mirror-pro.git
cd language-mirror-pro

# Create virtual environment
python3 -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Test the Model

```bash
# Test architecture
python -m ai_core.models.transformer

# Test training environment
python -m ai_core.training.environment
```

### 3. Train the Model (Optional)

```bash
# Quick training (10 minutes)
python scripts/train.py --num_updates 100

# Full training (2-4 hours)
python scripts/train.py --num_updates 2000 --save_interval 100
```

### 4. Start the Server

```bash
cd backend
python main.py
# Server runs at http://localhost:8000
```

### 5. Start the Frontend

```bash
cd frontend
npm install
npm run dev
# Frontend at http://localhost:3000
```

---

## ğŸ“Š Model Specifications

| Component | Details |
|-----------|---------|
| **Architecture** | Multi-Task Transformer |
| **Parameters** | ~12 Million |
| **Encoder Layers** | 6 |
| **Decoder Layers** | 4 |
| **Attention Heads** | 6 |
| **Hidden Dimension** | 384 |
| **Vocabulary Size** | 16,000 (multilingual BPE) |
| **Position Encoding** | Rotary (RoPE) |
| **Activation** | SwiGLU |
| **Normalization** | RMSNorm |

---

## ğŸ“ RL Training Details

### State Space
- User utterance (tokenized)
- Target language embedding
- Learner proficiency: [vocabulary, grammar, pronunciation, confidence, error_rate]

### Action Space
12 response types:
- Greeting, Gentle Correction, Direct Correction
- Encouragement, Simple Question, Complex Question
- Vocabulary Introduction, Grammar Explanation
- Pronunciation Tip, Cultural Note, Practice Prompt, Conversation

### Reward Function
```python
# Pedagogically-informed rewards
+1.0: Encouragement when confidence < 0.4
+1.2: Gentle correction for struggling learner
+0.8: Vocabulary intro for beginners
-0.3: Over-correction of advanced learner
-0.5: Causing learner frustration
```

### Curriculum Learning
- Starts with A1 (beginner) learners
- Advances difficulty after 5 successful episodes
- Reaches C2 (mastery) level learners

---

## ğŸ“ Project Structure

```
language-mirror-pro/
â”œâ”€â”€ ai_core/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ transformer.py      # ğŸ§  Custom transformer model
â”‚   â”‚   â””â”€â”€ tokenizer.py        # ğŸ”¤ BPE tokenizer
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â””â”€â”€ environment.py      # ğŸ® RL training environment
â”‚   â””â”€â”€ inference/
â”‚       â””â”€â”€ engine.py           # âš¡ Inference optimization
â”œâ”€â”€ backend/
â”‚   â””â”€â”€ main.py                 # ğŸš€ FastAPI server
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ ...                     # ğŸ’» React/Next.js app
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ train.py                # ğŸ“ˆ Training script
â”œâ”€â”€ checkpoints/                # ğŸ’¾ Model weights
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ”¬ Technical Innovations

### 1. Multi-Task Learning
Single model handles grammar correction, pronunciation scoring, and response generation simultaneously, with shared representations improving all tasks.

### 2. Pedagogical Reward Shaping
Rewards are designed based on language learning research:
- Gentle corrections for struggling learners
- Challenge increases for confident learners
- Engagement optimization

### 3. Curriculum Learning
Training starts with simulated beginner learners and progressively introduces advanced scenarios, improving sample efficiency.

### 4. Efficient Architecture
- RoPE for better length generalization
- SwiGLU activation (2x more efficient than GELU)
- RMSNorm (faster than LayerNorm)
- Weight tying between embedding and output layers

---

## ğŸ¯ Demo Scenarios

### Scenario 1: Grammar Correction
```
User: "Io sono fame"
Tutor: "Quasi perfetto! Si dice 'Ho fame'. In italiano usiamo 
        'avere' per la fame, non 'essere'!"
Translation: "Almost perfect! We say 'Ho fame'. In Italian we use 
              'to have' for hunger, not 'to be'!"
```

### Scenario 2: Encouragement
```
User: "Buongiorno, mi chiamo Marco"
Tutor: "Ottimo lavoro! La tua pronuncia sta migliorando molto! 
        Come stai oggi, Marco?"
Translation: "Great job! Your pronunciation is improving a lot! 
              How are you today, Marco?"
```

### Scenario 3: Practice Prompt
```
User: "Come si dice 'thank you'?"
Tutor: "In italiano diciamo 'grazie'! Proviamo insieme: 
        'Grazie mille per il tuo aiuto!'"
Translation: "In Italian we say 'grazie'! Let's try together: 
              'Thank you very much for your help!'"
```

---

## ğŸ‘¥ Team

Built for **AI Hackathon 2025**

---

## ğŸ“„ License

MIT License - See [LICENSE](LICENSE) for details.

---

<div align="center">

**Built with â¤ï¸ and PyTorch**

*No APIs were harmed in the making of this project*

</div>
